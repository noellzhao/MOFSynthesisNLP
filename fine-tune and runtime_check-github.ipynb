{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers --upgrade"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4qN_PhoEjVq",
    "outputId": "e30cf00d-606d-4a54-d9ae-a1080de4e8fa"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2o-CHas0nB2",
    "outputId": "b7719387-b890-434c-b756-127274033925"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXEXZAuHdDGr"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "random.seed(42)\n",
    "import re # for extract tuples\n",
    "from itertools import combinations # for generate combinations"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1LN69LHdPIb",
    "outputId": "1851bdb6-d3ca-48a7-c77f-083878b9d1ec"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fri Apr 25 06:13:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0             45W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ],
   "metadata": {
    "id": "w0koYgOAdgZE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c207f745-fb92-4fb4-c51d-f911f3c406d9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your runtime has 89.6 gigabytes of available RAM\n",
      "\n",
      "You are using a high-RAM runtime!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"acs_overall_not_synthesis_paragraph.txt\",'r',encoding='utf-8') as f:\n",
    "  not_synthesis = f.read()\n",
    "\n",
    "with open(\"acs_overall_synthesis_paragraph.txt\",'r',encoding='utf-8') as f:\n",
    "  is_synthesis = f.read()\n",
    "\n"
   ],
   "metadata": {
    "id": "B-dPE2BxWsJx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv(\"Training_SynParas.csv\")\n",
    "df_test = df_test[['Paragraph','if_synthesis']]\n",
    "df_test = df_test.rename(columns={'Paragraph': 'text', 'if_synthesis': 'label'})\n",
    "df_test = df_test.dropna()\n",
    "df_test = df_test.reset_index()"
   ],
   "metadata": {
    "id": "1OdUSer5YZU-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_no_syn = [record.rsplit(\"\\t\",1)[0] for record in not_synthesis.split('\\n')]\n",
    "training_syn = [record.rsplit(\"\\t\",2)[0] for record in is_synthesis.split('\\n')]\n",
    "\n",
    "# down-sampling non-synthesis paragraphs to 50-50\n",
    "random.shuffle(training_no_syn)\n",
    "training_no_syn = training_no_syn[:len(training_syn)]\n",
    "\n",
    "\n",
    "training_labels = []\n",
    "training_labels += [0]*len(training_no_syn)\n",
    "training_labels += [1]*len(training_syn)\n",
    "training_texts = training_no_syn + training_syn"
   ],
   "metadata": {
    "id": "OE7xZSTj3noS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = pd.DataFrame(columns=['text','label'])\n",
    "df_train['text'] = training_texts\n",
    "df_train['label'] = training_labels\n"
   ],
   "metadata": {
    "id": "6_PrlPsH3neM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ],
   "metadata": {
    "id": "tshM_azaYZOa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#df_train.to_csv(\"training_data.csv\",index=False)"
   ],
   "metadata": {
    "id": "cpVklfRoFaRM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(df_train)"
   ],
   "metadata": {
    "id": "JNQolOxDYZKI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 2. Tokenize the text\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=500)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# 3. Load pre-trained BERT for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "a13e3d9d233a42daa8a0dceb1e2c7f9f",
      "b3b5ba2eab8e4754928c33493b25bc49",
      "c7711e0194a64bd691bffa01c1012a2a",
      "fa42b9cc3c624ffd8a8714f71960337a",
      "f0b8c635571c49818477160b8a168389",
      "1ecf342e68a44b29a275af79cedbb48c",
      "1aa2341b453b498babaed5404476ea5e",
      "04d25ea9d435454d84ab08c221e329dd",
      "5e5c1d2081a14c05b45f07839c0aa2b4",
      "2943b8b933374a67a00904860d3178d5",
      "bde4250c204b47d0b419a47920ca9ce3"
     ]
    },
    "id": "lnEieaaWYZA7",
    "outputId": "4ccbd5e8-831b-4b88-c75a-45b89b2cb622"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_dataset_test = Dataset.from_pandas(df_test).map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_dataset_test = tokenized_dataset_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset_test.set_format(\"torch\")"
   ],
   "metadata": {
    "id": "XnzZr8_5G6Cf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"epoch\",\n",
    "    # Remove or comment out 'evaluation_strategy'\n",
    "    # evaluation_strategy=\"epoch\", # This line is causing the error\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=1e-5,\n",
    "    report_to=\"none\"  # Explicitly disable wandb reporting\n",
    ")"
   ],
   "metadata": {
    "id": "sp8RnDYs7bkG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset_test\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "3wfxL6Pi62fD",
    "outputId": "ee6d08fe-74d8-4cbc-d658-0cff44424ffd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='2688' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2688/2688 07:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2688, training_loss=0.07260230591609365, metrics={'train_runtime': 447.9522, 'train_samples_per_second': 47.992, 'train_steps_per_second': 6.001, 'total_flos': 5523790496220000.0, 'train_loss': 0.07260230591609365, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Save your model\n",
    "model.save_pretrained(\"./bert_binary_syn_classifier\")\n",
    "tokenizer.save_pretrained(\"./bert_binary_syn_classifier\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipXRGWMF6-sD",
    "outputId": "c6c44814-6d0f-40dc-e550-5203f3809edd"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./bert_binary_syn_classifier/tokenizer_config.json',\n",
       " './bert_binary_syn_classifier/special_tokens_map.json',\n",
       " './bert_binary_syn_classifier/vocab.txt',\n",
       " './bert_binary_syn_classifier/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import TextClassificationPipeline\n"
   ],
   "metadata": {
    "id": "roZAiuNgArzm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 1. Load model and tokenizer\n",
    "model_path = \"./bert_binary_syn_classifier\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 2. Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 3. Create the pipeline manually\n",
    "classifier = TextClassificationPipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    return_all_scores=True  # optional, shows probs for all labels\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAN4D8Wq-pyP",
    "outputId": "36f4d9f0-f0d6-45d1-bac6-dfee17483e92"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_binary_label(prediction_list):\n",
    "    top_label = max(prediction_list, key=lambda x: x['score'])['label']\n",
    "    return 1 if top_label == 'LABEL_1' else 0\n",
    "\n"
   ],
   "metadata": {
    "id": "OZnC12UK_F_A"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Run predictions\n",
    "val_texts = df_test['text'].tolist()\n",
    "val_labels = df_test['label'].tolist()\n",
    "\n",
    "pred_labels = classifier(val_texts,truncation=True, max_length=500)"
   ],
   "metadata": {
    "id": "B5seCeQOBbrM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pred_labels = [get_binary_label(pred) for pred in pred_labels]"
   ],
   "metadata": {
    "id": "amHgnOhRBd3L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['Not Synthesis', 'Is Synthesis']\n",
    "print(classification_report(val_labels, pred_labels, target_names=target_names))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kn6D1ycOJsj8",
    "outputId": "cf64e079-2a9d-49c0-e9d1-8fec5226e0d4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Synthesis       0.95      0.95      0.95       170\n",
      " Is Synthesis       0.88      0.88      0.88        69\n",
      "\n",
      "     accuracy                           0.93       239\n",
      "    macro avg       0.92      0.92      0.92       239\n",
      " weighted avg       0.93      0.93      0.93       239\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "AAz32LcIKBWr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "!zip -r ./bert_binary_syn_classifier.zip ./bert_binary_syn_classifier # Create a zip archive of the folder\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOI2GLGRKptx",
    "outputId": "744e16c3-bb8a-415c-a0f1-90378fcdcc30"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: bert_binary_syn_classifier/ (stored 0%)\n",
      "  adding: bert_binary_syn_classifier/config.json (deflated 49%)\n",
      "  adding: bert_binary_syn_classifier/vocab.txt (deflated 53%)\n",
      "  adding: bert_binary_syn_classifier/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_binary_syn_classifier/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_binary_syn_classifier/model.safetensors (deflated 7%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "files.download('./bert_binary_syn_classifier.zip') # Download the zip file\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "POSHQN9bKqoI",
    "outputId": "11ee4da8-810d-41d0-e3aa-f12c2755ae46"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_de605d3b-1425-4148-af12-895aff995c37\", \"bert_binary_syn_classifier.zip\", 405356064)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# --- Line of code to measure ---\n",
    "test = classifier(val_texts,truncation=True, max_length=500)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Runtime:\", end - start, \"seconds\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ui96UT1LzjR",
    "outputId": "824b6272-39b7-400c-f43a-3f29e79bddad"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Runtime: 2.9798362255096436 seconds\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Relation Extraction"
   ],
   "metadata": {
    "id": "5LMlGBekQNUf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# merge text paragraph to structured action graph\n",
    "with open(\"acs_overall_clear_output1225.txt\",'r',encoding='utf-8') as f:\n",
    "  acs_overall_clear_output = f.read()\n",
    "\n",
    "with open(\"codified_synthesis_action_graph.txt\",'r',encoding='utf-8') as f:\n",
    "  codified_synthesis_action_graph = f.read()\n"
   ],
   "metadata": {
    "id": "2FQysCVdNfTP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_paragraphs = acs_overall_clear_output.split('\\n')\n",
    "list_paragraphs[0].split('\\t',maxsplit=2)\n",
    "para_idx = []\n",
    "para_text = []\n",
    "for paragraph in list_paragraphs:\n",
    "  if paragraph.strip() == '':\n",
    "    continue\n",
    "  para_idx.append(paragraph.split('\\t',maxsplit=2)[2])\n",
    "  para_text.append(paragraph.split('\\t',maxsplit=2)[1])\n",
    "assert len(para_idx) == len(para_text)"
   ],
   "metadata": {
    "id": "JSIcNEXGQHKx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_graphs = codified_synthesis_action_graph.split('END\\n')\n",
    "graph_idx = []\n",
    "graph_relations = []\n",
    "num_error = 0\n",
    "for graph in list_graphs:\n",
    "  if graph.strip() == '':\n",
    "    continue\n",
    "  try:\n",
    "    graph_idx.append(graph.rsplit('\\t',maxsplit=1)[0])\n",
    "    graph_relations.append(graph.rsplit('\\t',maxsplit=1)[1].split('Entity-entity pairs related with each step, delimited by comma:\\n')[1])\n",
    "  except:\n",
    "    num_error += 1\n",
    "    print(graph)\n",
    "    continue\n",
    "assert len(graph_idx) == len(graph_relations)"
   ],
   "metadata": {
    "id": "1Xa_rhXCRzKp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_paragraphs = pd.DataFrame(columns=['idx','text'])\n",
    "df_paragraphs['idx'] = para_idx\n",
    "df_paragraphs['text'] = para_text\n",
    "\n",
    "df_graphs = pd.DataFrame(columns=['idx','relations'])\n",
    "df_graphs['idx'] = graph_idx\n",
    "df_graphs['relations'] = graph_relations\n",
    "\n",
    "# prompt: inner join df_paragraphs and df_graphs based on column 'idx'\n",
    "\n",
    "df_merged = pd.merge(df_paragraphs, df_graphs, on='idx', how='inner')"
   ],
   "metadata": {
    "id": "Ul6POEK9TIh_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Training Dataset for Relation Extraction"
   ],
   "metadata": {
    "id": "cAtBQNwEVWVX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pattern = r'\\(([^,]+),\\s*([^)]+)\\)'\n",
    "list_tuples = []\n",
    "for relations in df_merged['relations']:\n",
    "  # Find all matches and return as list of tuples\n",
    "  list_tuples += [re.findall(pattern, relations)]\n",
    "\n"
   ],
   "metadata": {
    "id": "T72L6reuVc3L"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "combo_tuples = []\n",
    "combo_labels = []\n",
    "for tuples in list_tuples:\n",
    "  tuple_set = set(tuples)\n",
    "\n",
    "  # Flatten the list of tuples and create a set to remove duplicates\n",
    "  unique_items = set([item for pair in tuples for item in pair])\n",
    "\n",
    "  # Generate all unique 2-element combinations (unordered)\n",
    "  combinations_list = list(combinations(unique_items, 2))\n",
    "\n",
    "  combo_tuples += [combinations_list]\n",
    "\n",
    "  # Check if each combination or its reverse exists in original tuples\n",
    "  exists_flags = []\n",
    "  for a, b in combinations_list:\n",
    "      if (a, b) in tuple_set or (b, a) in tuple_set:\n",
    "          exists_flags.append(1)\n",
    "      else:\n",
    "          exists_flags.append(0)\n",
    "  combo_labels += [exists_flags]\n"
   ],
   "metadata": {
    "id": "MkQ1dyd9WdTw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_original_texts = df_merged['text'].tolist()"
   ],
   "metadata": {
    "id": "2CS7fI36eEh4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "labeled_texts_for_training = []\n",
    "for index in range(len(list_original_texts)):\n",
    "  original_text = list_original_texts[index]\n",
    "  # List of entity pairs to label\n",
    "  target_pairs = combo_tuples[index]\n",
    "\n",
    "  # Store the labeled versions\n",
    "  labeled_versions = []\n",
    "\n",
    "  for chem, detail in target_pairs:\n",
    "    text = copy.deepcopy(original_text)\n",
    "\n",
    "    # Escape both parts for regex safety\n",
    "    chem_pattern = re.escape(chem)\n",
    "    detail_pattern = re.escape(detail)\n",
    "\n",
    "    # Replace the first occurrence of chem\n",
    "    text = re.sub(chem_pattern, f\"<ENT>{chem}</ENT>\", text, count=1)\n",
    "\n",
    "    # Replace the first occurrence of detail\n",
    "    text = re.sub(detail_pattern, f\"<ENT>{detail}</ENT>\", text, count=1)\n",
    "\n",
    "    # Save result\n",
    "    labeled_versions.append(text)\n",
    "  labeled_texts_for_training += [labeled_versions]\n",
    "\n",
    "\n",
    "assert len(labeled_texts_for_training) == len(combo_labels)"
   ],
   "metadata": {
    "id": "Gft8Dg7db9Em"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flatten both text and label list"
   ],
   "metadata": {
    "id": "57u51tRUfsOA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "final_training_texts = []\n",
    "final_labels = []\n",
    "for index in range(len(labeled_texts_for_training)):\n",
    "  final_training_texts += labeled_texts_for_training[index]\n",
    "  final_labels += combo_labels[index]\n",
    "assert len(final_training_texts) == len(final_labels)"
   ],
   "metadata": {
    "id": "ewfX7bEEcCo5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# gathering everything into one single data frame\n",
    "df = pd.DataFrame(columns=['text','label'])\n",
    "df['text'] = final_training_texts\n",
    "df['label'] = final_labels"
   ],
   "metadata": {
    "id": "iWdDP1gMfCJO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upsampling Training Set\n",
    "\n",
    "While upsampling the training dataset, the test dataset is untouched"
   ],
   "metadata": {
    "id": "GZE2vWG2hmNl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 1: Train-test split (stratified to preserve label distribution)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# Step 2: Separate majority and minority classes in the training set\n",
    "train_majority = train_df[train_df['label'] == 0]\n",
    "train_minority = train_df[train_df['label'] == 1]\n",
    "\n",
    "# Step 3: Downsample the majority class to match the size of the minority class\n",
    "train_majority_downsampled = train_majority.sample(n=len(train_minority), random_state=42)\n",
    "\n",
    "# Step 4: Combine and shuffle\n",
    "train_balanced = pd.concat([train_majority_downsampled, train_minority], ignore_index=True)\n",
    "train_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Final datasets:\n",
    "# - train_balanced: training set with balanced classes (via downsampling)\n",
    "# - test_df: test set with original class distribution\n",
    "# Now you have:\n",
    "# - train_balanced: training data with balanced classes\n",
    "# - test_df: untouched test set with original class distribution\n"
   ],
   "metadata": {
    "id": "o-oqeGN4hMZY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start Training Experiment"
   ],
   "metadata": {
    "id": "Q7NuTIPPh5jM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 1. convert pandas data frame into DataSet format\n",
    "dataset_train = Dataset.from_pandas(train_balanced)\n",
    "dataset_test = Dataset.from_pandas(test_df)\n",
    "\n",
    "# 2. Tokenize the text\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=500)\n",
    "\n",
    "tokenized_dataset_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_train = tokenized_dataset_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset_train.set_format(\"torch\")\n",
    "\n",
    "tokenized_dataset_test = dataset_test.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = tokenized_dataset_test.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset_test.set_format(\"torch\")\n",
    "\n",
    "# 3. Load pre-trained BERT for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440,
     "referenced_widgets": [
      "2746ad1ae0354c54a42a3aabbc150f2a",
      "7d84f8df7423491d9fdf4349a24d2c98",
      "71c8fcf991c346a4a9eea3678e4b8eb0",
      "182c426c22614af29247f36f1a8b9e04",
      "4b28be9db3fe4765bcd772d5e4f57868",
      "560916285e514947ba0fd8fae00a4e09",
      "32117acf9e454cd5bd1e8442a480f23d",
      "b6fee8b035f0493a99963fbc7739d0a4",
      "c60d2e31e9144421ba9e4000c1afdb84",
      "20974d9db7744eb195272c328dca7fe7",
      "1bd14e33650945a49e8d72bf444b3681",
      "872ed60f6c344006859b65ced882023f",
      "1329ee343b73412ab02a01bdd85573d0",
      "f69327d647b54136825950fbe0e5a29b",
      "b4914a7b3a7441bbb73f77fa6be35bf0",
      "acea2b9be49947b4ad7d4add053d3339",
      "39bee30c7f5f421e9b5dfeadb6bc83c4",
      "294d6eacb8b54e23b12af75541099b88",
      "e3018df7009b41739d9d6aae6c4500bb",
      "259dc980dce64d828ac72f90a63c3303",
      "1f7c97d29ec042458b1b30bbd03d9305",
      "db48a25282974d97aede5911f456a159",
      "c3ef02aefbd64dc4ba7ee749b7f3b814",
      "87a9b506c0804e36aa427e2ff89ba94e",
      "fcf7baad41194c94a3fad093e15852ef",
      "8b8e96a13390427994c948240241a324",
      "6ae8989210104143aa5ec9746a3de2fc",
      "7ce62942659040888f24631b79161f66",
      "f2d8341230344e819104ea645a5ea14d",
      "2f23dcdba0b44c5d86dd55271117d361",
      "e56bf4ed6ebe4c2bb404cf8e5b199090",
      "448df704586e44f7b35a8f9551408173",
      "1de25bbef0d440de9d8efdddc31945e6",
      "f902e3d85533491b810763322bb971b5",
      "2b6aab4ad12c43b983b6e2b8ac3924a2",
      "0f3918795b584f9180b47568139977c2",
      "bf5d5d780ad14f28989016ed86f9240b",
      "ad87ecc1f0b440c9a151ba3678a71f03",
      "7b293c7299e042589f02b152ef54c8ba",
      "ce50e26e221945f0a77096cb99240762",
      "6641102c5f5f400b954ef393955c4e20",
      "e52a4d091a0b491382e09d0841bb7f93",
      "a4125e2ec8de4a68b3f3838cd66337d4",
      "8859b4368abb4d42a2c57dc1d6481707",
      "edf1185ec02f4a91b8fcea73c6fc5be9",
      "f0090be7e74b479699fe231fde3a09c4",
      "1e1a10e8c0fb43efb2f8a87fa175a62e",
      "86ff20caf2d34e1fac7c42fe7a47d98c",
      "78ef5e62a2014c5989841c5d745f537e",
      "3678baae052f49e49f075cfa51da68c7",
      "ef6fad2abdc140c9999a08ec1fcc6639",
      "8d49a6dbc0d14d759dd6f05d9629f9c5",
      "dba4f465e3f54501937f0587faea6257",
      "6bc6666e3a5e4fa897344ec55a8d8281",
      "69c44a898224469bab99b79f7b9aa44b",
      "2034612d970847818a16fdef64490e95",
      "49b027d0660e4c58b3c368bee84ede51",
      "edb268eee899456b801bd93c7dcab269",
      "09864f25281e4fc0861cd5582cc5fcd5",
      "ccf0820e843347d7ac0232ac7a953958",
      "04064ff0855843f1ade7f2e335d76cd7",
      "1c42033efb1e41fa96cd9fbaa25250de",
      "2807205376e8480894fce875a566f01a",
      "773189bb21ed433aa1f1f6d0e0297601",
      "8375f71422814c3498b02c7f59bdaf8b",
      "a1de440d170d427d8b811d114dedabbd",
      "a1f8e351cafe4b2bb858bdf7df6e869d",
      "4f142c06eaa846d191c15723b119bcfa",
      "a6bd1e7386d44cddafb9ecdff7ba1846",
      "4e45d35716ad4e3489ece86e9f31f5ba",
      "4c15d9fc114c496ebcc30031bfc7aca2",
      "c1ba073bd9b544bd89fb481f9bd465b5",
      "b82a325f38db43e0bf090c890578b2d5",
      "fd7d287bf86c455e9e3ba6180e8ae6e2",
      "82d1365861314feba38560d1f72eaeb4",
      "697734b5858141dba97f14547f15302e",
      "a6b9db9f5a26451ea0d72bfcdfb4917c"
     ]
    },
    "id": "wOk_Cw5Thia6",
    "outputId": "3aa7a1a6-258f-4df8-968f-d8f1f0b59d25"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Widget output removed for GitHub preview]"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 4. Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,\n",
    "    eval_strategy=\"epoch\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    # Remove or comment out 'evaluation_strategy'\n",
    "    # evaluation_strategy=\"epoch\", # This line is causing the error\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5,\n",
    "    report_to=\"none\"  # Explicitly disable wandb reporting\n",
    ")"
   ],
   "metadata": {
    "id": "8kVMq7_wjOoB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'f1': f1_score(labels, preds, average='weighted'),\n",
    "        'precision': precision_score(labels, preds, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(labels, preds, average='weighted')\n",
    "    }"
   ],
   "metadata": {
    "id": "r0owjgeenYzL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# training starts\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_train,\n",
    "    eval_dataset=tokenized_dataset_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "MpzNdekeka0g",
    "outputId": "64677e40-47f5-4e74-f9e5-2855ebfb85b9"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16544' max='16544' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16544/16544 2:24:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.301549</td>\n",
       "      <td>0.884312</td>\n",
       "      <td>0.910965</td>\n",
       "      <td>0.960084</td>\n",
       "      <td>0.884312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205500</td>\n",
       "      <td>0.272479</td>\n",
       "      <td>0.889304</td>\n",
       "      <td>0.914526</td>\n",
       "      <td>0.961901</td>\n",
       "      <td>0.889304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.266146</td>\n",
       "      <td>0.892414</td>\n",
       "      <td>0.916696</td>\n",
       "      <td>0.962693</td>\n",
       "      <td>0.892414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.252912</td>\n",
       "      <td>0.923452</td>\n",
       "      <td>0.937704</td>\n",
       "      <td>0.965324</td>\n",
       "      <td>0.923452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16544, training_loss=0.21869189022703373, metrics={'train_runtime': 8642.0201, 'train_samples_per_second': 30.624, 'train_steps_per_second': 1.914, 'total_flos': 6.800187447984e+16, 'train_loss': 0.21869189022703373, 'epoch': 4.0})"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. Save your model\n",
    "model.save_pretrained(\"./bert_relation_classifier\")\n",
    "tokenizer.save_pretrained(\"./bert_relation_classifier\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34noC1RJkfCf",
    "outputId": "4ba1c2a1-296e-44d6-b8cf-789b5e537879"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./bert_relation_classifier/tokenizer_config.json',\n",
       " './bert_relation_classifier/special_tokens_map.json',\n",
       " './bert_relation_classifier/vocab.txt',\n",
       " './bert_relation_classifier/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "!zip -r ./bert_relation_classifier.zip ./bert_relation_classifier # Create a zip archive of the folder\n",
    "files.download('./bert_relation_classifier.zip') # Download the zip file\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "MAsjMXyjkrza",
    "outputId": "9b44ded7-e984-404a-a9f7-5f7941055b7c"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  adding: bert_relation_classifier/ (stored 0%)\n",
      "  adding: bert_relation_classifier/vocab.txt (deflated 53%)\n",
      "  adding: bert_relation_classifier/model.safetensors (deflated 7%)\n",
      "  adding: bert_relation_classifier/config.json (deflated 49%)\n",
      "  adding: bert_relation_classifier/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_relation_classifier/tokenizer_config.json (deflated 75%)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_b8f2d1a0-8cc8-4a81-82dd-81e5335afeb9\", \"bert_relation_classifier.zip\", 405357907)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "G6CuaZHgomJj",
    "outputId": "9e049b91-d0be-49a9-c391-9abe7e59177d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5234' max='5234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5234/5234 15:25]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'eval_loss': 0.2529124617576599, 'eval_accuracy': 0.9234517184551816, 'eval_f1': 0.9377040771157283, 'eval_precision': 0.9653242369578728, 'eval_recall': 0.9234517184551816, 'eval_runtime': 925.4658, 'eval_samples_per_second': 180.964, 'eval_steps_per_second': 5.656, 'epoch': 4.0}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ol3wjDHhsAE9"
   }
  }
 ]
}